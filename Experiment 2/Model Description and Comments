Model Description
•	A Multi-Layer Perceptron (MLP) consists of an input layer, one or more hidden layers, and an output layer.
•	Unlike a single-layer perceptron, an MLP can solve non-linearly separable problems like XOR.
•	The model uses:
o	Sigmoid activation function for non-linearity.
o	Forward propagation to compute outputs.
o	Backpropagation to update weights using gradient descent.



Explanation:

Neural Network for XOR Function:
This program implements a neural network with one hidden layer to learn the XOR Boolean function.

Key Insights:
- XOR is a non-linear problem that a single-layer perceptron cannot solve.
- By adding a hidden layer with activation functions, the network can learn the XOR pattern.

Training Process:
- Forward pass: Calculates predictions.
- Backward pass: Updates weights based on prediction errors.

Parameter Tuning:
- The learning rate (lr) and the number of hidden units can be adjusted for better performance.
- Loss decreases over epochs as the network learns the correct XOR mappings.
